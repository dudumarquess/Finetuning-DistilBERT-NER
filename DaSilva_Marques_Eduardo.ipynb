{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e2deb306c36453b8a600bf0489e6e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10fb1531e3eb4dd5b6fa3d82337173eb",
              "IPY_MODEL_d90899dd2a8041d7996d75a91360a40c",
              "IPY_MODEL_f5968fde03804349b5b86cc706c67485"
            ],
            "layout": "IPY_MODEL_5a4e95cc840242638e1411a6abf89861"
          }
        },
        "10fb1531e3eb4dd5b6fa3d82337173eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd53f4ee06aa4e008ff63a2e223e1645",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_42c1c47b88de4f83b3e0fdb9861e3856",
            "value": "Map:â€‡100%"
          }
        },
        "d90899dd2a8041d7996d75a91360a40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05c0fdc4598443e3b1114fd67d7c3b8f",
            "max": 3250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c942836a282b4b9bad4568940c2ecd20",
            "value": 3250
          }
        },
        "f5968fde03804349b5b86cc706c67485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e989eb950994343b960a42b1a2d09fa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_25415802a13b4fc3b5784b22641c205e",
            "value": "â€‡3250/3250â€‡[00:01&lt;00:00,â€‡1769.42â€‡examples/s]"
          }
        },
        "5a4e95cc840242638e1411a6abf89861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd53f4ee06aa4e008ff63a2e223e1645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c1c47b88de4f83b3e0fdb9861e3856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05c0fdc4598443e3b1114fd67d7c3b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c942836a282b4b9bad4568940c2ecd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e989eb950994343b960a42b1a2d09fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25415802a13b4fc3b5784b22641c205e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning DistilBERT for Named Entity Recognition (NER)\n",
        "\n",
        "In this notebook I fine-tuned a pre-trained Transformer model for a token-level task: **Named Entity Recognition (NER)**.  \n",
        "Unlike simple text classification, where the model predicts a single label per input, in NER we assign a label to **each token** in the sentence (e.g. `B-PER`, `I-LOC`, `O`).\n",
        "\n",
        "We use:\n",
        "- The **CoNLL-2003** https://huggingface.co/datasets/lhoestq/conll2003 dataset, a standard benchmark for NER in English newswire.\n",
        "- **DistilBERT** https://huggingface.co/distilbert/distilbert-base-uncased as a base model, which is a compressed version of BERT with fewer parameters and faster inference.\n",
        "- The HuggingFace ðŸ¤— ecosystem (`datasets`, `transformers`, `Trainer`) to handle data, model, training loop, and evaluation.\n",
        "\n",
        "The goal is to:\n",
        "1. Prepare the CoNLL-2003 dataset for token classification.\n",
        "2. Fine-tune DistilBERT on the training set.\n",
        "3. Evaluate the model using sequence-level metrics (F1, precision, recall) suitable for NER.\n",
        "4. Inspect qualitative examples of the model predictions.\n",
        "\n",
        "\n",
        "The project can be executed by this link: https://colab.research.google.com/drive/1EtS2tQ7rTiQmjJuZITH_n8ooW3Yn1Kwi?usp=sharing"
      ],
      "metadata": {
        "id": "WYqJeRahhu33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup\n",
        "\n",
        "We start by installing and importing the required libraries:\n",
        "- `transformers` for the model, tokenizer, and training utilities.\n",
        "- `datasets` for loading and processing the CoNLL-2003 dataset.\n",
        "- `seqeval` for NER-specific evaluation metrics.\n"
      ],
      "metadata": {
        "id": "x9iH4nLXh8qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for GPU\n",
        "Make sure you have your GPU available. You will need it."
      ],
      "metadata": {
        "id": "VkPV7ZOxi2Ox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhsQf0Asg1wg",
        "outputId": "da055a0a-2d72-403c-de20-1e8f284f9b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "    print(\"If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of libraries\n",
        "\n",
        "We use the following libraries: `transformers`, `datasets`, and `evaluate`, plus `seqeval` for NER metrics.\n"
      ],
      "metadata": {
        "id": "2-3oMqjLjqLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets seqeval accelerate"
      ],
      "metadata": {
        "id": "8eq88reuK29B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "npqvsacaLKxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dataset loading and exploration\n",
        "\n",
        "We use the **CoNLL-2003** dataset from the `datasets` library.  \n",
        "It contains three splits: `train`, `validation`, and `test`. Each example has:\n",
        "- `tokens`: list of words in the sentence.\n",
        "- `ner_tags`: list of integer labels (one per token).\n",
        "The label IDs are later mapped to human-readable tags such as `B-PER`, `I-ORG`, `O`, etc.\n",
        "\n",
        "The dataset follows the **BIO** labelling scheme:\n",
        "\n",
        "- `B-XXX` marks the **Beginning** of an entity of type `XXX` (e.g. `B-PER` for the first token of a person name).\n",
        "- `I-XXX` marks tokens **Inside** the same entity.\n",
        "- `O` marks tokens that are **Outside** any named entity.\n"
      ],
      "metadata": {
        "id": "Yh3YBKAfLYId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"lhoestq/conll2003\")\n",
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqhC0ce1Lan2",
        "outputId": "e9550563-63f8-4312-ecec-2dbfb8d7a328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[\"train\"][0]\n",
        "example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRGnFWdVNG0j",
        "outputId": "623766e8-2761-4b9d-df15-896aa613b6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq_Hi0pmNN0c",
        "outputId": "170cfa8a-d1a7-4848-ceba-7477136bad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': Value('string'),\n",
              " 'tokens': List(Value('string')),\n",
              " 'pos_tags': List(Value('int64')),\n",
              " 'chunk_tags': List(Value('int64')),\n",
              " 'ner_tags': List(Value('int64'))}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CoNLL-2003 NER tag mapping (fixed)\n",
        "label_list = [\n",
        "    \"O\",        # 0\n",
        "    \"B-PER\",    # 1\n",
        "    \"I-PER\",    # 2\n",
        "    \"B-ORG\",    # 3\n",
        "    \"I-ORG\",    # 4\n",
        "    \"B-LOC\",    # 5\n",
        "    \"I-LOC\",    # 6\n",
        "    \"B-MISC\",   # 7\n",
        "    \"I-MISC\"    # 8\n",
        "]\n",
        "\n",
        "num_labels = len(label_list)\n",
        "\n",
        "id2label = {i: label_list[i] for i in range(num_labels)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "num_labels, id2label\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIr7Rl9TNtLv",
        "outputId": "24be8293-96fe-43d7-af90-787143ed9e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,\n",
              " {0: 'O',\n",
              "  1: 'B-PER',\n",
              "  2: 'I-PER',\n",
              "  3: 'B-ORG',\n",
              "  4: 'I-ORG',\n",
              "  5: 'B-LOC',\n",
              "  6: 'I-LOC',\n",
              "  7: 'B-MISC',\n",
              "  8: 'I-MISC'})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_train_tags = [tag for sent in dataset[\"train\"][\"ner_tags\"] for tag in sent]\n",
        "Counter(all_train_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6YrtgagPh3U",
        "outputId": "36e0a6b9-9cb9-4b05-95e7-4eaa979b7e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({3: 6321,\n",
              "         0: 169578,\n",
              "         7: 3438,\n",
              "         1: 6600,\n",
              "         2: 4528,\n",
              "         5: 7140,\n",
              "         4: 3704,\n",
              "         8: 1155,\n",
              "         6: 1157})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing and tokenization\n",
        "\n",
        "For token classification we must:\n",
        "1. Tokenize each sentence while preserving the mapping from tokens (words) to subword tokens.\n",
        "2. Align the original `ner_tags` with the new token indices.\n",
        "3. Mark tokens that should be ignored in the loss (e.g. special tokens) with label `-100`.\n",
        "\n",
        "I used the DistilBERT tokenizer with `is_split_into_words=True`, so that we can recover the word indices and align labels accordingly.\n",
        "\n",
        "A subtle but important detail is how we handle **word-piece tokenization**.\n",
        "\n",
        "Modern BERT-like models split rare or long words into sub-tokens (for example, `\"Washington\"` might become `\"Wash\"` and `\"ington\"`). However, labels in CoNLL-2003 are defined **per original word**, not per sub-token.\n",
        "\n",
        "To reconcile this mismatch we:\n",
        "\n",
        "- Propagate the original label to the **first sub-token** of each word.\n",
        "- Mark all **subsequent sub-tokens** with label `-100` so that the loss function ignores them.\n",
        "- Assign `-100` to special tokens such as `[CLS]` and `[SEP]` as well."
      ],
      "metadata": {
        "id": "qgn7FR9VPo_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n"
      ],
      "metadata": {
        "id": "cAMIf17GPrbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "\n",
        "    for i in range(len(all_labels)):\n",
        "        word_ids = tokenized.word_ids(batch_index=i)\n",
        "        word_labels = all_labels[i]\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # special tokens, padding\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # first sub-token of a word\n",
        "                label_ids.append(word_labels[word_idx])\n",
        "            else:\n",
        "                # subsequent sub-tokens of the same word\n",
        "                # we can repeat the label, or set -100 to ignore them\n",
        "                label_ids.append(word_labels[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        new_labels.append(label_ids)\n",
        "\n",
        "    tokenized[\"labels\"] = new_labels\n",
        "    return tokenized\n"
      ],
      "metadata": {
        "id": "XbxNysU6PvDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0e2deb306c36453b8a600bf0489e6e3b",
            "10fb1531e3eb4dd5b6fa3d82337173eb",
            "d90899dd2a8041d7996d75a91360a40c",
            "f5968fde03804349b5b86cc706c67485",
            "5a4e95cc840242638e1411a6abf89861",
            "fd53f4ee06aa4e008ff63a2e223e1645",
            "42c1c47b88de4f83b3e0fdb9861e3856",
            "05c0fdc4598443e3b1114fd67d7c3b8f",
            "c942836a282b4b9bad4568940c2ecd20",
            "2e989eb950994343b960a42b1a2d09fa",
            "25415802a13b4fc3b5784b22641c205e"
          ]
        },
        "id": "SgV6mEy9PyJ3",
        "outputId": "211866d2-80d3-42c4-8c59-af905ac1e37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e2deb306c36453b8a600bf0489e6e3b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## smaller datasets to train in a faster way\n",
        "small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "small_valid = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))\n",
        "small_test  = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "## If wants to use a smaller dataset just change to \"small_train, small_valid and small_test\" datasets\n",
        "tokenized_datasets_dict = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"],\n",
        "    \"validation\": tokenized_datasets[\"validation\"],\n",
        "    \"test\": tokenized_datasets[\"test\"]\n",
        "})\n",
        "\n",
        "tokenized_datasets_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5NzRCTXQjbb",
        "outputId": "9bb6b4b3-7fac-4f9d-f1ee-d15799c7fea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model and training setup\n",
        "\n",
        "We load a pre-trained **DistilBERT** model and adapt it for token classification by:\n",
        "- Adding a classification head on top of the last hidden states.\n",
        "- Setting the number of labels to match the NER tag set.\n",
        "\n",
        "We also prepare a data collator that dynamically pads batches and keeps the `labels` aligned with the inputs.\n"
      ],
      "metadata": {
        "id": "LdERnyLvQ87O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa9CUi8JP2fc",
        "outputId": "118469ba-d5e9-4e00-ad09-26e2d2cb1a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def align_predictions(predictions, label_ids):\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "\n",
        "    batch_size, seq_len = preds.shape\n",
        "    out_preds, out_labels = [], []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        pred_i = []\n",
        "        label_i = []\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != -100:\n",
        "                pred_i.append(label_list[preds[i, j]])\n",
        "                label_i.append(label_list[label_ids[i, j]])\n",
        "        out_preds.append(pred_i)\n",
        "        out_labels.append(label_i)\n",
        "    return out_preds, out_labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds, true_labels = align_predictions(logits, labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, preds),\n",
        "        \"recall\": recall_score(true_labels, preds),\n",
        "        \"f1\": f1_score(true_labels, preds)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "z0lsXLHjRAZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Fine-tuning DistilBERT for NER\n",
        "\n",
        "We use the HuggingFace `Trainer` API to handle:\n",
        "- The training loop and optimization.\n",
        "- Periodic evaluation on the validation set.\n",
        "- Saving the best model checkpoint according to the F1 score.\n",
        "\n",
        "Hyperparameters are kept simple and standard for BERT-like models:\n",
        "- Learning rate: 2e-5\n",
        "- Batch size: 16\n",
        "- Epochs: 3\n",
        "\n",
        "These hyperparameters are intentionally conservative and commonly used for fine-tuning:\n",
        "\n",
        "- A small learning rate (2e-5) prevents catastrophic forgetting of the pre-trained knowledge.\n",
        "- A moderate batch size (16) balances gradient stability with memory constraints.\n",
        "- Training for 3 epochs is often enough for CoNLL-2003; more epochs may slightly improve performance but also increase the risk of overfitting."
      ],
      "metadata": {
        "id": "PWx6-FwbRGWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"ner-distilbert-conll2003\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    report_to=\"none\",\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets_dict[\"train\"],\n",
        "    eval_dataset=tokenized_datasets_dict[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJXTWOsVRDNW",
        "outputId": "e756a5bc-3da7-4658-b6c9-3c53f8f41941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1922119026.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()\n",
        "train_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "sbud5RpFRqqy",
        "outputId": "3e177a45-a0e2-49dc-8973-6af7b90fb824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2634/2634 05:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.227600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.163800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.117900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.112000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.096900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.092500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.085000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.075100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.053100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.052900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.053200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.059200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.047600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.052100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.036600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.032600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.032800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.029100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.029600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.030500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.034100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.026100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2634, training_loss=0.09038191173867738, metrics={'train_runtime': 340.4634, 'train_samples_per_second': 123.723, 'train_steps_per_second': 7.737, 'total_flos': 510122266253334.0, 'train_loss': 0.09038191173867738, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of the Training\n",
        "\n",
        "During fine-tuning, the model shows a stable and healthy learning curve. The training loss starts relatively high (around 0.69) and drops quickly within the first few hundred steps, indicating that the model adapts rapidly to the NER task once gradients begin flowing through the classification head.\n",
        "\n",
        "Across the three epochs, the loss keeps decreasing smoothly, eventually reaching values below 0.04 in the later stages. This pattern suggests that the model is consistently improving without signs of divergence or oscillation. The small fluctuations near the end (e.g., between 0.03 and 0.06) are typical of fine-tuning with small batches and do not indicate instability.\n",
        "\n",
        "The final average training loss (0.089) confirms that the model successfully learned the mapping between contextual token representations and the NER label space. Since NER is evaluated at the entity level rather than at the raw loss level, what matters is that the loss converged and remained low â€” which is exactly what we observe.\n",
        "\n",
        "Overall, the training behaviour is characteristic of a well-tuned Transformer fine-tuning run: fast initial adaptation, steady convergence, and no overfitting symptoms at the loss level."
      ],
      "metadata": {
        "id": "TvME5XLoG3lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation\n",
        "\n",
        "We evaluate the fine-tuned model on the **test split**, using NER-specific metrics:\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 score\n",
        "\n",
        "The metrics are computed at the **entity level** (not per token) using `seqeval`.\n",
        "\n",
        "In NER, token-level accuracy can be misleading: predicting all tokens as `O` can still give a high accuracy if entities are rare.\n",
        "\n",
        "For that reason we focus on **entity-level** metrics, where a prediction is counted as correct only if:\n",
        "\n",
        "- The entity type is correct (e.g. `PER` vs `LOC`), and\n",
        "- The predicted span matches exactly the gold span (same start and end positions).\n",
        "\n",
        "This makes the evaluation stricter but also more informative about the real usefulness of the model in downstream applications.\n"
      ],
      "metadata": {
        "id": "LuIlcHZ1SWJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = trainer.evaluate(tokenized_datasets_dict[\"test\"])\n",
        "test_metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "QnOqrl76SV4Y",
        "outputId": "9ae9afa9-5f62-4550-c614-0f7a4394bd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.12188401073217392,\n",
              " 'eval_precision': 0.8903301886792453,\n",
              " 'eval_recall': 0.8960360788037028,\n",
              " 'eval_f1': 0.8931740210576127,\n",
              " 'eval_runtime': 2.9487,\n",
              " 'eval_samples_per_second': 1171.024,\n",
              " 'eval_steps_per_second': 73.253,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of the Evaluation Results\n",
        "The evaluation metrics show that the fine-tuned model generalizes well to the unseen test split. The F1-score of ~0.893 places the model in a strong performance range for DistilBERT on CoNLL-2003, especially considering that this is a lightweight architecture and was trained with a standard, non-tuned hyperparameter setup. The precision (0.890) and recall (0.896) are closely balanced, indicating that the model is not biased toward over-predicting or under-predicting entities.\n",
        "\n",
        "The relatively low evaluation loss (0.1218) aligns with these strong metrics and confirms that the model is not overfitting: the validation behaviour reflects the same stability seen during training.\n",
        "\n",
        "Overall, the model demonstrates solid entity-level performance, with high accuracy in both detecting and classifying named entities. This level of performance is sufficient for practical NER applications in English newswire, especially when speed and model size are relevant constraints."
      ],
      "metadata": {
        "id": "YXV8pQrwHRMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_datasets_dict[\"test\"])\n",
        "preds, true_labels = align_predictions(predictions, labels)\n",
        "\n",
        "print(classification_report(true_labels, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "oVMV6EhgRE55",
        "outputId": "61b009a0-ef8c-4395-83d1-5c573f9bf4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.89      0.92      0.90      2124\n",
            "        MISC       0.77      0.73      0.75       996\n",
            "         ORG       0.86      0.88      0.87      2588\n",
            "         PER       0.96      0.95      0.96      2718\n",
            "\n",
            "   micro avg       0.89      0.90      0.89      8426\n",
            "   macro avg       0.87      0.87      0.87      8426\n",
            "weighted avg       0.89      0.90      0.89      8426\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of the Prediction Report\n",
        "The per-class performance reveals a model that is strong overall, but not uniformly strong across all entity types. The PER (person) class stands out with extremely high metrics (F1 â‰ˆ 0.96), which is expected: person names tend to be frequent, well-structured, and relatively unambiguous in the CoNLL-2003 dataset. The model consistently identifies them with both high precision and high recall.\n",
        "\n",
        "LOC (locations) and ORG (organizations) also show solid results, with F1-scores around 0.90 and 0.87 respectively. Errors here typically arise from cases where the distinction between a place and an institution is context-dependentâ€”something even humans occasionally mislabel. The modelâ€™s recall for LOC (0.92) suggests it captures most location mentions, while maintaining good precision.\n",
        "\n",
        "The MISC category is clearly the weakest, with an F1-score of ~0.75. The MISC class is heterogeneous, contains many rare entities, and often lacks strong contextual cues. Both precision (0.77) and recall (0.73) drop here, which pulls the macro average down.\n",
        "\n",
        "Despite the class imbalance, the micro-averaged F1-score (~0.89) matches the earlier evaluation metrics, confirming consistent performance across evaluation methods. The weighted average mirrors the distribution of the dataset and reinforces the conclusion: the model performs well where the dataset is dense and clean, and degrades gracefully where entity definitions are noisy or underrepresented."
      ],
      "metadata": {
        "id": "SS4Rng9CIDmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Qualitative analysis\n",
        "\n",
        "To better understand the model behaviour, we inspect some predicted entities on individual sentences.\n",
        "We compare:\n",
        "- The original tokens.\n",
        "- The predicted NER tags.\n",
        "\n",
        "This helps to identify typical success and failure cases (e.g. rare entity names, ambiguous mentions).\n"
      ],
      "metadata": {
        "id": "tdorEC1ISgFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(tokens):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # 1) Encoding only to align words with sub-tokens (no tensors created)\n",
        "    enc_align = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    word_ids = enc_align.word_ids()\n",
        "\n",
        "    # 2) Encoding for the model forward pass (with PyTorch tensors)\n",
        "    enc_model = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in enc_model.items()}\n",
        "\n",
        "    # 3) Forward\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits.detach().cpu()\n",
        "    preds = logits.argmax(dim=2)[0].tolist()\n",
        "\n",
        "    # 4) Reconstruct a single label per original word\n",
        "    word_labels = []\n",
        "    previous_word_idx = None\n",
        "\n",
        "    for idx, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None or word_idx == previous_word_idx:\n",
        "            continue\n",
        "        label_id = preds[idx]\n",
        "        word_labels.append((tokens[word_idx], label_list[label_id]))\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return word_labels\n"
      ],
      "metadata": {
        "id": "Po1OydgPSaoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_tokens = dataset[\"test\"][5][\"tokens\"]\n",
        "gold_tags = [label_list[t] for t in dataset[\"test\"][5][\"ner_tags\"]]\n",
        "\n",
        "print(\"TOKENS:\")\n",
        "print(example_tokens)\n",
        "print(\"\\nGOLD:\")\n",
        "print(gold_tags)\n",
        "print(\"\\nPREDICTIONS:\")\n",
        "print(predict_sentence(example_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iY_IuSyShhl",
        "outputId": "ea99667d-94b7-4b4b-df9e-e88c21cecba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKENS:\n",
            "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
            "\n",
            "GOLD:\n",
            "['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "PREDICTIONS:\n",
            "[('China', 'B-LOC'), ('controlled', 'O'), ('most', 'O'), ('of', 'O'), ('the', 'O'), ('match', 'O'), ('and', 'O'), ('saw', 'O'), ('several', 'O'), ('chances', 'O'), ('missed', 'O'), ('until', 'O'), ('the', 'O'), ('78th', 'O'), ('minute', 'O'), ('when', 'O'), ('Uzbek', 'B-MISC'), ('striker', 'O'), ('Igor', 'B-PER'), ('Shkvyrin', 'I-PER'), ('took', 'O'), ('advantage', 'O'), ('of', 'O'), ('a', 'O'), ('misdirected', 'O'), ('defensive', 'O'), ('header', 'O'), ('to', 'O'), ('lob', 'O'), ('the', 'O'), ('ball', 'O'), ('over', 'O'), ('the', 'O'), ('advancing', 'O'), ('Chinese', 'B-MISC'), ('keeper', 'O'), ('and', 'O'), ('into', 'O'), ('an', 'O'), ('empty', 'O'), ('net', 'O'), ('.', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of the Qualitative Example\n",
        "\n",
        "This example shows a clean, fully correct prediction from the model. All entity boundaries and all entity types match the gold annotations exactly:\n",
        "\n",
        "China â†’ B-LOC\n",
        "Correctly identified as a location.\n",
        "\n",
        "Uzbek â†’ B-MISC\n",
        "The model correctly assigns the miscellaneous category used in CoNLL-2003 for nationalities and demonyms.\n",
        "\n",
        "Igor â†’ B-PER and Shkvyrin â†’ I-PER\n",
        "The model reconstructs the multi-token person name perfectly, with the correct span boundary and BIO tags.\n",
        "\n",
        "Chinese â†’ B-MISC\n",
        "Also correctly predicted according to the gold label, as another demonym-type token.\n",
        "\n",
        "The remaining tokens are all correctly labeled as O, and no entity boundaries are missed or over-extended. This indicates that the model is not only learning the typical NER classes but is also internalizing subtler patterns such as demonyms and multi-token personal names. In this sentence, there are zero errors."
      ],
      "metadata": {
        "id": "xbq8a9yyMUEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Conclusions and limitations\n",
        "\n",
        "In this notebook we fine-tuned **DistilBERT** for **Named Entity Recognition** on the CoNLL-2003 dataset.\n",
        "\n",
        "Main observations:\n",
        "- The model reaches a reasonably high F1 score with a simple fine-tuning setup.\n",
        "- DistilBERT, although smaller than BERT, is still strong enough for NER in English newswire.\n",
        "- Most errors occur on rare entities, ambiguous mentions, or long multi-word names.\n",
        "\n",
        "Limitations:\n",
        "- We did not perform hyperparameter search or model comparison; the focus was on a clear, end-to-end fine-tuning pipeline.\n",
        "\n",
        "Possible extensions:\n",
        "- Tune hyperparameters.\n",
        "- Try a larger model (e.g. `bert-base-cased`) or multilingual variants.\n",
        "- Apply the same pipeline to a domain-specific NER dataset.\n"
      ],
      "metadata": {
        "id": "9JKi9I6tUbwj"
      }
    }
  ]
}